import math
import numpy as np
import scipy as sp
import scipy.spatial
import multiprocessing.dummy as mp
import ROOT

class kNNDensityEstimator(object):

    # Initialize the kNN density estimator
    #  -data		Array of data points
    #  -rotate		Evaluate the distances in the metric of the covariance matrix
    #  -nthreads	Number of threads available
    #  -norm		Normalisation factor (i.e. transmission, default to 1.)
    def __init__(self, data, rotate=True, nthreads=1, norm=1.):

	# Initialize the base members
	self.data = data
	self.n = len(data)
	self.dim = data.shape[1]
	self.norm = norm
	self.nthreads = nthreads

	# Set the optimal number of neighbors
	self.k = int(pow(self.n/self.norm, 4./(4.+self.dim)))

    	# If requested, get the metric of the space and rotate all of the
	# points to fit a simple Euclidean L2 metric (used by the KD tree)
	self.rotate = rotate
	self.scale = 1.
	self.cov = np.cov(data.transpose())
	if rotate:
	    # a) Get the scaling factor (generalised radius of the RMS ellipse)
	    self.scale = math.sqrt(np.linalg.det(self.cov))

	    # b) Find the eigenvalues and eigenvectors of the covariance matrix
	    #    Sort them so that each eigenvec is associated with the right eigenval
	    eigval, eigvec = np.linalg.eigh(self.cov)
	    sort_perm = eigval.argsort()
	    eigval.sort()
	    eigvec = eigvec[:, sort_perm]

	    # c) Find the metric to rotate the points into a Euclidean set
	    invLsqrt = np.full([self.dim, self.dim], 0.)
	    for i in range(self.dim):
	        invLsqrt[i, i] = pow(eigval[i], -0.5)

	    Ut = np.matrix(eigvec).transpose()
	    self.metric = invLsqrt*Ut

	    # d) Rotate all the points 
	    for i in range(self.n):
	        x = self.metric*np.matrix(data[i]).transpose()
	        self.data[i] = x.transpose()

	# Finally, initialize the underlying KD tree on the rotated data
	# The tree does not deep copy, do not touch data (!)
	self.kdtree = sp.spatial.cKDTree(self.data)

	# Initialize a container for the contour levels
	self.levels = []

    # Evaluate the kNN density in x
    def eval(self, x):

	# Rotate the vector in the same metric, if necessary
	if self.rotate:
	    x = np.array(self.metric*np.matrix(x).transpose()).flatten()

        return self.norm*self.density(x)/self.scale

    # Returns the density profile as two arrays (values and stat errors)
    #  -npoints	Number of points contained in the graph (number of steps)
    #  -bsr	Use bootstrap resampling to evaluate uncertainties
    #  -bsr_n	Number of iterations of the bootstrap resampling
    def profile(self, npoints=1000, bsr=False, bsr_n=10):

	# Set the levels of the training set if they have not been set yet
	if not len(self.levels):
	    self.set_levels()

	# Use the requested uncertainty calculation, produce profiles
	values, errors = [0. for i in range(npoints)], [0. for i in range(npoints)]
        if not bsr:
	    # Take the quantiles as the estimate, the Gaussian error as the uncertainty
	    for i in range(npoints):
	    	alpha = (float(i+1)/(npoints+1))
		level = 0.
		if alpha < self.norm:
	    	    level = np.quantile(self.levels, 1.-alpha/self.norm)

	    	values[i] = level
	    	errors[i] = level*self.level_uncertainty(alpha)
	else:
	    # Bootstrap the data, evaluate the profiles for each new sample
	    baseline = self.data
	    baseline_levels = self.levels
            levels = np.empty((npoints, bsr_n))
	    for j in range(bsr_n):
		# Generate a new sample
		sample = np.empty((self.n, self.dim))
		for i in range(self.n):
		    k = np.random.randint(self.n)
		    sample[i] = baseline[k]

		# Reinitialize the KD tree on the new sample, reset the levels
		self.__init__(sample, self.rotate, self.nthreads, self.norm)
		self.set_levels()

		# Evaluate the array of levels of the new sample	        
	    	for i in range(npoints):
	            alpha = (float(i+1)/(npoints+1))
		    levels[i][j] = 0.
		    if alpha < self.norm:
	    	        levels[i][j] = np.quantile(self.levels, 1.-alpha/self.norm)

	    # Take the mean as the estimate, the rms as the uncertainty
	    for i in range(npoints):
		values[i] = np.mean(levels[i])
		errors[i] = np.std(levels[i])

	    # Reset the data
	    self.__init__(baseline, self.rotate, self.nthreads, self.norm)
	    self.levels = baseline_levels

	# Return
        return values, errors

    # Returns the density profile as a graph
    #  -npoints	Number of points contained in the graph (number of steps)
    #  -bsr	Use bootstrap resampling to evaluate uncertainties
    #  -bsr_n	Number of iterations of the bootstrap resampling
    def profile_graph(self, npoints=1000, bsr=False, bsr_n=10):

	# Initialize the graph
	profile = ROOT.TGraphErrors(npoints)
        profile.SetTitle(";Fraction #alpha;#rho_{#alpha}")
	profile.GetYaxis().SetTitleOffset(1)

	# Set the points
	values, errors = self.profile(npoints, bsr, bsr_n)
	for i in range(npoints):
	    alpha = (float(i+1)/(npoints+1))
	    profile.SetPoint(i, alpha, values[i])
	    profile.SetPointError(i, 0., errors[i])

	# Return
	return profile

    # Returns a lower dimensional section of the space at x
    #  -axes	List of axes ids to produce a Poincare section of
    #  -x	d-point that the Poincare section contains
    #  -mins	Minimums in each of the axes
    #  -maxs	Maximums in each of the axes
    def section(self, axes, x=[], mins=[], maxs=[]):

	# Check that the dimension of the section is supported and sensible
	secdim = len(axes)	
	if not secdim:
	    raise ValueError('No axis(es) specified')
	if secdim > 2:
	    raise ValueError('Sections of dimension larger than 2 not supported')
	for axis in axes:
	    if axis >= self.dim:
		raise ValueError('Axis id %d is not compatible with space dimension %d'\
									 %(axis, self.dim))

	# If an intersection point is provided, check that it is of the right dimension
	# If not provided, take the point at the origin of the space
  	if len(x) and len(x) != self.dim:
	    raise ValueError('The intersection point must be of dimension %d'%self.dim)
	if not len(x):
	    x = np.full(self.dim, 0.)
	
	# Checks if the boundaries are of the right dimension
	# If not provided, take the 4 sigma interval around the mean
	if len(mins) and len(mins) != secdim:
	    raise ValueError('The minimum point must be of dimension %d'%self.dim)
	if len(maxs) and len(maxs) != secdim:
	    raise ValueError('The maximum point must be of dimension %d'%self.dim)
	if not len(mins) or not len(maxs):
	    mins = np.empty(secdim)
	    maxs = np.empty(secdim)
	    for i in range(secdim):
		sample = self.data.transpose()[axes[i]]
		mean = np.mean(sample)
		std = math.sqrt(self.cov[axes[i]][axes[i]])
		mins[i] = mean-4*std
		maxs[i] = mean+4*std

	# Create a histogram
	section = None
	if secdim == 1:
	    nbins = 100
    	    section = ROOT.TH1F("section_%d"%(axes[0]),\
				";x_{%d};#rho(#bf{x})"%(axes[0]),\
				nbins, mins[0], maxs[0])
    	    for i in range(nbins):
		pos = x
		pos[axes[0]] = section.GetXaxis().GetBinCenter(i+1)
		val = self.eval(pos)
		section.SetBinContent(i+1, val)

	if secdim == 2:
	    nbins = 50
    	    section = ROOT.TH2F("section_%d%d"%(axes[0], axes[1]),\
				";x_{%d};x_{%d};#rho(#bf{x})"%(axes[0], axes[1]),\
				nbins, mins[0], maxs[0], nbins, mins[1], maxs[1])
    	    for i in range(nbins):
		for j in range(nbins):
		    pos = x
		    pos[axes[0]] = section.GetXaxis().GetBinCenter(i+1)
		    pos[axes[1]] = section.GetYaxis().GetBinCenter(j+1)
	    	    val = self.eval(pos)
	    	    section.SetBinContent(i+1, j+1, val)

        return section

    # Returns the density in the Euclidean metric in x
    def density(self, x):
	vol = self.volume(x)
	return float(self.k)/self.n/vol

    # Returns the volume of the d-ball centred in x of radius R_k
    def volume(self, x):
	dist = self.distance(x)
	return pow(math.sqrt(math.pi)*dist, self.dim)/sp.special.gamma(self.dim/2+1)

    # Returns the distance to the k^th closest point, R_k
    def distance(self, x):
	dists, ids = self.kdtree.query(x, self.k)
	return dists[-1]

    # Sets the contour levels (evaluate the density in all of the points)
    def set_levels(self):
	self.levels = np.empty(self.n)
	# Single-threaded approach
	if self.nthreads == 1:
	    for i in range(self.n):
	        set_level(i)

	# Multi-threaded approach
	else:
	    pool = mp.Pool(processes=self.nthreads)
	    pool.map(self.set_level, range(self.n))
	    pool.close()
	    pool.join()

    # Set a specific contour level (in one of the points)
    def set_level(self, i):
	self.levels[i] = self.norm*self.density(self.data[i])/self.scale

    # Returns the Gaussian uncertainty associated with a contour level
    def level_uncertainty(self, frac):
	return pow(4./(frac*(1-frac)), 1./4)/math.sqrt(self.n)
